{
  "name": "Hogwildpp",
  "tagline": "HogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent",
  "body": "HogWild++ Experiment Code\r\n===============================================================================\r\n\r\nHogWild++ aims to improve the scalability of HogWild! stochastic gradient\r\ndescent algorithm on multi-socket (NUMA) machines. In HogWild++, to reduce\r\ninter-socket communication and coherence miss, worker threads are grouped into\r\nseveral \"clusters\" with configurable size, and in each cluster threads share\r\nthe same model vector.  Each cluster only exchanges model information with its\r\nneighbour(s) to reduce communication overhead, and no centralized model vector\r\nis maintained.  For more details about this algorithm please refer to the\r\nfollowing paper:\r\n\r\n```\r\nHogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent\r\nHuan Zhang, Cho-Jui Hsieh and Venkatesh Akella\r\n```\r\n\r\nIn our implementation, clusters are organized in a logical directional ring,\r\nand there is a single token passing along the ring. The cluster holding the\r\ntoken is able to communicate with the next cluster on the ring and exchange\r\nmodel information, and other clusters keep updating their own models. If you\r\nare interested in implementing other (potentially more efficient) topologies\r\nplease read the `How to modify` section below.\r\n\r\n\r\nHow to build\r\n----------------------\r\n\r\nHogWild++ requires `libnuma`. On Debian based systems you can install `libnuma`\r\nusing the following command:\r\n\r\n```\r\nsudo apt-get install libnuma-dev\r\n```\r\n\r\nThen run `make` to build, and the following binaries will be built in `bin` folder:\r\n\r\n* svm: Basic SVM example in HogWild!, unchanged, and it is used as the\r\n  performance baseline.\r\n\r\n* numasvm: Implementataion of SVM using the HogWild++ algorithm.\r\n\r\n* convert: Convert TSV files into binary files. Assumes the rows and columns in\r\n  the TSV file are indexed starting at 0.\r\n\r\n* convert_matlab: Converts TSV files into binary files. Assumes the rows and\r\n  columns are indexed starting at 1.\r\n\r\n* unconvert: Converts a binary file into a TSV file. The TSV file will be\r\n  indexed starting at 0.\r\n\r\nData Preparation\r\n----------------------\r\n\r\nDatasets can be downloaded from [LIBSVM website](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)\r\nhowever you need to convert them to the TSV (Tab Separated Value) format that HogWild! uses.\r\nWe provide a script `covert2hogwild.py` to convert LIBSVM format to TSV format.\r\nTo reduce data loading time, you should also convert TSV to binary format.\r\n\r\nThe following commands show how to download and prepare the RCV1 dataset.\r\nNote that for RCV1 we swapped the downloaded training and test set because the \"test set\"\r\nis actually larger.\r\n```\r\nmkdir data && cd data\r\n# prepare the training set\r\nwget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_test.binary.bz2\r\nbunzip2 rcv1_test.binary.bz2\r\npython ../convert2hogwild.py rcv1_test.binary rcv1_train.tsv\r\n../bin/convert rcv1_train.tsv rcv1_train.bin\r\n# prepare the test set\r\nwget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2\r\nbunzip2 rcv1_train.binary.bz2\r\npython ../convert2hogwild.py rcv1_train.binary rcv1_test.tsv\r\n../bin/convert rcv1_test.tsv rcv1_test.bin\r\n# Done\r\ncd ..\r\n\r\n```\r\n\r\n\r\nWe have prepared binary files used in the experiments of our paper.\r\nYou can download these datasets here:\r\n\r\n[http://jaina.cs.ucdavis.edu/datasets/classification_compressed/](http://jaina.cs.ucdavis.edu/datasets/classification_compressed/)\r\n\r\nYou only need to download .bin.xz files. To save downloading time these files\r\nare compressed. Please decompress them using the `xz` utility before use.\r\n\r\n\r\nHow to run\r\n----------------------\r\n\r\nHogWild++ adds three new parameters to HogWild! executable. \r\n\r\n* `cluster_size`: the number of threads in each cluster (referred as `c` in our\r\n  paper). Threads within the same cluster share the same model vector. When the\r\n  cluster size is the same as the total number of threads, HogWild++ is\r\n  equvalent to HogWild!. The best cluster size depends on dataset\r\n  characteristics. As a starting point, you can set the cluster size as the\r\n  total number of physical cores on a single socket.\r\n\r\n* `update_delay`: after one cluster finishes synchronization with the next\r\n  cluster, it waits for `update_delay` more updates (inner SGD iterations)\r\n  before passing the token to the next cluster. This delay is referred as\r\n  `\\tau_0` in our paper.  If you have less than 4 clusters, 64 is the\r\n  recommended value. If you have more clusters, you can try a smaller update\r\n  delay.\r\n\r\n* `tolerence`: When one cluster writes its model delta to the next cluster, it\r\n  will skip some model elements if they only changed very little. The\r\n  threshold is given by the `tolerance` parameter. Usually you should set it\r\n  between 1e-2 to 1e-6.\r\n\r\nThe following command runs the RCV1 dataset prepared above for 150 epochs with\r\n40 threads, with a cluster size of 10, step size of 5e-01, step decay of 0.928\r\nand update delay of 64:\r\n\r\n```\r\nbin/numasvm --epoch 150 --binary 1 --stepinitial 5e-01 --step_decay 0.928 --update_delay 64 --cluster_size 10 --split 40 data/rcv1_train.bin data/rcv1_test.bin\r\n```\r\n\r\nDon't forget to change the number of threads (the --splits argument) and the\r\ncluster size to reflect your hardware configuration. For example, if you\r\nhave a dual-socket 20-core machine, you can change splits to 20 and cluster\r\nsize to 10 or 5. \r\n\r\nIf you specify more threads than the total number of physical cores available,\r\nhyper-threading cores will be used as well. This will maximize the computation\r\npower of your machine. However please note that the number of clusters will\r\nstill be calculated using the total number of physical cores.  For example, if\r\nyou have a dual-socket 12-core machine (24 threads with hyperthreading) and you\r\nrun HogWild++ with a cluster size of 6, and 24 worker threads, only 2 clusters\r\nwill be created instead of 4.\r\n\r\nWe provide two python scripts, `collect_svm.py` and `collect_numasvm.py` to run\r\nthe baseline HogWild! and our improved algorithm, HogWild++, using the same\r\ndatasets and parameters in our paper.  Before running these scripts, make sure\r\nyou have downloaded all datasets and put them inside the `data` directory.  To\r\nrun a quick experiment on your machine, just run `python collect_svm.py` to\r\ncollect HogWild! results and `python collect_numasvm.py` to collect HogWild++\r\nresults. Results of runs will be saved in `svm_mmdd-hhmmss` and\r\n`numasvm_mmdd-hhmmss` folders where `mmdd-hhmmss` represents date and time.\r\nYou can change the scripts to customize all parameters, like number of threads,\r\ncluster size, step size, etc.\r\n\r\nHow to modify\r\n----------------------\r\n\r\nIf you are interested in changing HogWild++ and explore more possibilities,\r\nyou can start by reading the following source files:\r\n\r\n* `src/numasvm_main.cpp`: this file contains the `main` function for `numasvm`.\r\n  Also, function `CreateNumaClusterRoundRobinRingSVMModel` creates the model\r\n  synchronization ring. If you are interested in changing the synchronization\r\n  topology you should change this function.\r\n\r\n* `src/numasvm/svm_exec.hxx`: The SVM solver. Function `ModelUpdate` contains\r\n  the SGD update rule and model synchronization procedure.\r\n\r\n* `hazytl/include/hazy/thread/thread_pool-inl.h`: An enhanced thread pool\r\n  implementation supporting CPU topology detection and affinity assignment.\r\n\r\nAdditional Information\r\n----------------------\r\n\r\nIf your have any questions or comments, please open an issue on Github,\r\nor send an email to ecezhang@ucdavis.edu. We appreciate your feedback.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}